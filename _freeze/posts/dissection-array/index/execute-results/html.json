{
  "hash": "7c5e007a5f565e0a8638e4ce4801198c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Dissection of a Tensor\"\ndate: 2025-08-04\ntoc: true\ntoc-title: \"Table of Contents\"\ntoc-location: left\n---\n\n## Introduction: What Is It an Array?\n\nBefore going through what is an array and why they are so much used in Computer Science or Machine Learning, we will have to do a quick excursus into how variables are stored in memory.\n\nWhen we declare a variable, that variable will be stored into the RAM (random access memory) of our computer. From this moment that variable, let's say $x$ will have an address that will point to that memory location, a variable that stores the address of another one is called *pointer*.\n\n::: {#3d0e7ed7 .cell execution_count=1}\n``` {.python .cell-code}\nx = 4\nprint(hex(id(x)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0x102a23010\n```\n:::\n:::\n\n\nHere we have assigned the value $4$ to the variable x, what we will have is:\n\n- Id: it is the unique identity of that variable\n- Hexadecimal address: where that variable is located\n\nThe address is represented in hexadecimal format with 0-9 and A-F where each byte has an address. For example a two digit number as a limit of FF or 255 in decimal system which is the same as 8 bits in binary.\n\nWe can see that if we copy a variable to another one, the new one is basically a pointer to the old address. However, if a different variable is initialized with the same value as $x$, they will have a different address\n\n::: {#5a262b47 .cell execution_count=2}\n``` {.python .cell-code}\ny = x\nprint(id(y)==id(x))\nprint(hex(id(y)) == hex(id(x)))\n\nz=4\nprint(hex(id(x)) == hex(id(z)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrue\nTrue\nTrue\n```\n:::\n:::\n\n\n### What about Arrays\n\nArrays are objects composed of elements of the same type and stored in continuous memory cells (homoguous memory). Computationally speaking they are more efficient than lists as we can access their elements just by using the location as a single pointer, since the first element will define the memory location of the others, while lists have a different pointer for each element.\n\n::: {#e92ce0e2 .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport sys\n\n# Create a Python list and a NumPy array with the same content\npy_list = [1, 2, 3, 4, 5]\nnp_array = np.array([1, 2, 3, 4, 5])\n\nprint(\"Python list memory addresses:\")\nfor i, item in enumerate(py_list):\n    print(f\"  Element {i}: value={item}, address={id(item)}\")\n\nprint(\"\\nNumPy array memory layout:\")\nprint(f\"  Base address of array: {np_array.__array_interface__['data'][0]}\")\nprint(\"  Item size in bytes:\", np_array.itemsize)\nfor i in range(len(np_array)):\n    address = np_array.__array_interface__['data'][0] + i * np_array.itemsize\n    print(f\"  Element {i}: value={np_array[i]}, address={address}\")\n\nprint(\"\\nTotal memory used by list elements:\", sum(sys.getsizeof(x) for x in py_list))\nprint(\"Total memory used by NumPy array:\", np_array.nbytes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPython list memory addresses:\n  Element 0: value=1, address=4339150768\n  Element 1: value=2, address=4339150800\n  Element 2: value=3, address=4339150832\n  Element 3: value=4, address=4339150864\n  Element 4: value=5, address=4339150896\n\nNumPy array memory layout:\n  Base address of array: 105553160355888\n  Item size in bytes: 8\n  Element 0: value=1, address=105553160355888\n  Element 1: value=2, address=105553160355896\n  Element 2: value=3, address=105553160355904\n  Element 3: value=4, address=105553160355912\n  Element 4: value=5, address=105553160355920\n\nTotal memory used by list elements: 140\nTotal memory used by NumPy array: 40\n```\n:::\n:::\n\n\nHere we have print the number of bytes needed to store the list or the array and we can see the difference.\n\n### An Important Property of Arrays: Strided View\n\nNow we have seen how arrays are stored and why it is more efficient than other data structure, it is important to discuss about a cool application of their continguous memory: the a strided representation.\n\nBasically, each array $n$ of shape $(I, J)$ will be stored in memory as a continuous sequence of number, its elements, and the shape will just clarify how to view the array.\n\nBasically we can define a view of our array as a final shape and an number of elements in the line before moving to the next row, column, called strides and expressed in bits (8 bits for example).\n\n::: {#b1baec56 .cell execution_count=4}\n``` {.python .cell-code}\nx = np.arange(6).reshape((2, 3))\nprint(x)\nprint(x.strides)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[0 1 2]\n [3 4 5]]\n(24, 8)\n```\n:::\n:::\n\n\nHere we have defined a two dimensional array that has to move 24 bits before changing row and 8 bits before changing column. With this information we can change the view just by changing the shape and the stride view:\n\n::: {#fd8ebc0b .cell execution_count=5}\n``` {.python .cell-code}\nnew_shape = (3, 2)\nnew_stride = (16, 8)\nnew_x = np.lib.stride_tricks.as_strided(x, new_shape, new_stride)\nprint(new_x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[0 1]\n [2 3]\n [4 5]]\n```\n:::\n:::\n\n\n## What about Tensors?\n\n## Concrete Example: Building a Convolutional Kernel from Scratch\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}