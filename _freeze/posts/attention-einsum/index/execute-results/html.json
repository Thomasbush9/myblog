{
  "hash": "4c0df52c7a3e2968c943cf1bfdb8c2fa",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Understanding Attention with Einsum\"\ndate: 2025-07-17\nimage: images/selfattention.png\ntoc: true\ntoc-title: \"Table of Contents\"\ntoc-location: left\n---\n\n## Introduction\n\nThe attention mechanism is arguably one of the most impactful innovations in the field of machine learning, if no the most. Its influence has extended beyond computer science, influencing many disciplines (AlphaFold, the field-changing model for protein structure prediction, is heavily dependent on the attention mechanism). As of this writing, the paper 'Attention Is All You Need', which introduced the transformer architecture based solely on attention mechanisms, has received over $185,000$ citations.\n\nTo put that into perspective: the combined citation count of Darwin's 'On the Origin of Species', Hubel and Wiesel's seminal work on the visual system, and Chomsky's 'Syntactic Structures', a foundational text in linguistics, is still less than half of 'Attention Is All You Need'.\n\nI still remember the first time that I encountered attention few years ago, it was during the first class of my Master in Cognitive Sciences and AI. I must say that it has not been love at first sight, actually it was quite the opposite. I found the idea of Query, Key and Values confusing and it took me a while to really grasp the core idea behind the attention mechanisms and why it works so well.\n\nWith this blog I aim to give you the insights that helped me understanding attention so that you can start build it yourself. Thus, I will first explain the core idea of attention, the difference between other representations and then I will implement it from scratch using a PyTorch and Einsum (which is a very powerful library for linear algebra operations.)\n\n## The Core Idea of Attention and Why It Works\n\nThe attention mechanisms core concept is fairly simple if expressed with words. If you have an input sequence $N$ and you want to know how each element of your sequence relates to another one you should compare the similarity between the two, then you can scale the value of each element in order to reflect its global relationships with the other elements. Basically, you are saying that the meaning of an element of your sequence is reflected by its similarity with the others.\n\nIn the diagram we can observe the full attention mechanism:\n\n![Self-Attention Diagram](images/selfattention.png){width=90% fig-align='center'}\n\nNow, this was actually the appropriate definition of 'Self-attention' as we are comparing the sequence with itself, there are other kinds of attention, but I believe that it's helpful to start with self attention first as it is more intuitive and it's the most used mechanism nowadays.\n\n\n### What does really change between attention and MLP?\n\nThe main difference of attention compared to other traditional architectures, such as multi layer perceptron, is that attention does not model the inner representations based on the weights/output relation, rather it modifies them based on the relations between inputs of the sequence and the outputs.\n\n::: {.columns}\n\n::: {.column width=\"50%\"}\n![Attention Output](images/attention_com.png)\n:::\n\n::: {.column width=\"50%\"}\n![MLP Output](images/mlp_comparison.png)\n:::\n\n:::\n\n\nThis difference is clear if we consider that an attention layer does not change particurarly compared to a MLP. Attention has weights only in the linear layers that creates the embeddings\n\n### Computational Efficiency on GPUs\n\nThe popularity of the attention mechanism is partly due to its extreme efficiency in processing input sequences. Specifically, attention allows us to compute a summary of the entire sequence for each element, in parallel. This makes it far more scalable than traditional sequence models like RNNs, which process inputs sequentially and are harder to parallelize.\n\nBefore diving into how attention is computed on a GPU, it's helpful to understand how GPUs work in general. When we define a vector, such as vect_1, a special function called *kernel* is launched on the GPU. The kernel takes each element of the vector and assigns it to a separate thread, which is run by a core in the GPU.\n\n\n![gpu schematic](images/gpu_diagram.png){fig-align='center'}\n\nNow, if we compute the dot product between two vectors, the GPU doesn't process it as one big operation. Instead, the kernel divides the  computation into many smaller tasks, one for element, and assigns them to different cores. Each thread multiples one element from vect_1 with the corresponding element from the second vector. Finally, the results are combined using a reduction operation (sum), returning the final dot product.\n\nThis kind of parallelism is what gives GPUs their advantage over CPUs for task like attention, where operation across large sequences can be run simultaneously instead of one after the other.\n\nWhen applying this to attention, it's clear how much more efficient a forward pass can be compared to a recurrent neural network (RNN) processing the same input sequence. In attention, each element of the sequence can be handled in parallel because the computations for different positions are independent of one another. This allows the model to assign different parts of the attention mechanism to separate GPU cores, achieving high parallelism and speed.\nIn contrast, an RNN must compute each step sequentially, since the output at each time step depends on the result from the previous one. This dependency makes it difficult to parallelize, significantly slowing down the forward pass.\n\n![Attention vs RNNs](images/attention_rnn.png){fig-align='center'}\n\n## Implementation\n\nTo develop a multi-head attention module, we only need to import a couple of python's libraries, mostly to facilitate the backward pass during training, but I strongly encourage everyone to first build these modules from scratch to fully understand how they works (add link).\n\n::: {#f322e868 .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nfrom torch import nn\nimport math\n```\n:::\n\n\nAttention starts with the creation of three embeddings (queries, keys and values) generated from the same sequence input going through three different linear layers that map the sequence to the attention dimension $c$. This step is crucial as it's the only step where there are trainable parameters in this module, other than in the output layer. So in the code we define the three linear layers, and during the forward pass they create the query, key and value embedding. In the figure below you can see how we create the three embedding starting from the same sequence.\n\n![Embedding Creation](images/linear_embd_attention.png){fig-align='center'}\n\nThe next step is to compute the affinities between the queries and the keys, this step is crucial as it's the main switch from a normal feed forward neural network and attention. Here we are generating a matrix where each element of the sequence is compared to all the others. The comparison is obtained through dot product, which produces higher values if two vectors are pointing in the same direction. In the code we compute the dot product by using Einsum, which is a very handy library to compute linear algebra operation (add link), basically we are considering only the last dimension for the query and key vector and we are generating a matrix of shape $b, c, c$, where $b$ stands for batch dimension.\n\n![Compute affinities](images/attention_affinities.png){fig-align='center'}\n\nOnce we have completed this step the attention values are nearly computed. The last thing that we have left is to convert our affinities into weights using the softmax function, for those who do not remember it the softmax converts a vector into a probability distribution where the elements of the vector sum up to one. The formula is: $\\sigma(\\vect(z))_i = \\frac{\\exp(z_i)}{\\sum^K_{j=1} \\exp(z_j)}$.\n\n![Generate attention weights](images/attention_soft.png){fig-align='center'}\n\nFinally we compute the attention values by scaling the value vector by the attention weights and we return the output after passing it through a linear to map the attention dimension back to the same value of the input dimension.\n\n![Output Generation](images/output_attention.png){fig-align='center'}\n\nNow that we went through each step in detail we can put all of them together to built the initial diagram into a single python class called SelfAttentionModule. Below there is the full code to generate it.\n\n::: {#163c0957 .cell execution_count=2}\n``` {.python .cell-code}\n# we generate a random input\nx = torch.randn((12, 5, 10))\n#we then create a class:\nclass SelfAttentionModule(nn.Module):\n    def __init__(self, x_in, c):\n        super().__init__()\n        self.c = c\n        # we define the three linear layers\n        self.linear_q = nn.Linear(x_in, c)\n        self.linear_k = nn.Linear(x_in, c)\n        self.linear_v = nn.Linear(x_in, c)\n        self.linear_o = nn.Linear(c, x_in)\n\n    def forward(self, x:torch.Tensor)->torch.Tensor:\n        # we apply the linear transformations\n        Q = self.linear_q(x)\n        # scale Q\n        Q = Q / math.sqrt(self.c)\n        K = self.linear_k(x)\n        V = self.linear_v(x)\n        # calculate affinities:\n        affinities = torch.einsum('...kj, ...zi->...kz',Q, K)\n        # convert to weights\n        attn_weights = torch.softmax(affinities, dim=-1)\n        # get the attn_values:\n        attn_values = torch.einsum('...kz, ...zc->...kc', attn_weights, V)\n        attn_values = self.linear_o(attn_values)\n        return attn_values, attn_weights\n```\n:::\n\n\n## Visualization\n\nTo visualize how attention really works, we are going to generate the attention values for a random sentence and plot them. We need first to import a pre-trained model so that we have meaningful input vectors for each token of our sentence. We are going to use DistilBert from the Transformers library (add link).\n\n::: {#04ce8665 .cell execution_count=3}\n``` {.python .cell-code}\nfrom transformers import AutoTokenizer, AutoModel\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nmodel_name = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name, output_attentions=True)\n```\n:::\n\n\n::: {#884319f8 .cell execution_count=4}\n``` {.python .cell-code}\nsentence = \"The cat is fluffy and soft\"\ninputs = tokenizer(sentence, return_tensors=\"pt\")\n\n# Forward pass\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# Extract attention matrices (layer 0, head 0)\nattentions = outputs.attentions  # List of layers -> [batch, heads, tokens, tokens]\nattention_matrix = attentions[0][0, 0]  # First layer, first head\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nDistilBertSdpaAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n```\n:::\n:::\n\n\nWe first divide our sentence into tokens and convert the tokens into embeddings. Then the model will generate the attention values which we can plot.\n\n::: {#f114ac27 .cell execution_count=5}\n``` {.python .cell-code}\ntokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\nplt.figure(figsize=(10, 8))\nsns.heatmap(attention_matrix, xticklabels=tokens, yticklabels=tokens, cmap=\"viridis\")\nplt.title(\"Attention Matrix (Layer 0, Head 0)\")\nplt.xlabel(\"Key\")\nplt.ylabel(\"Query\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=761 height=671}\n:::\n:::\n\n\n## Conclusion and Suggested Material\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}