{
  "hash": "251b988c83288597e474e1fea054a5a3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Dissection of a Tensor\"\ndate: 2025-08-04\ntoc: true\ntoc-title: \"Table of Contents\"\ntoc-location: left\njupyter: myblog\n---\n\n## Introduction: What Is It an Array?\n\nBefore going through what is an array and why they are so much used in Computer Science or Machine Learning, we will have to do a quick excursus into how variables are stored in memory.\n\nWhen we declare a variable, that variable will be stored into the RAM (random access memory) of our computer. From this moment that variable, let's say $x$ will have an address that will point to that memory location, a variable that stores the address of another one is called *pointer*.\n\n::: {#65fb118f .cell execution_count=1}\n``` {.python .cell-code}\nx = 4\nprint(hex(id(x)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0x1050a62c8\n```\n:::\n:::\n\n\nHere we have assigned the value $4$ to the variable x, what we will have is:\n\n- Id: it is the unique identity of that variable\n- Hexadecimal address: where that variable is located\n\nThe address is represented in hexadecimal format with 0-9 and A-F where each byte has an address. For example a two digit number as a limit of FF or 255 in decimal system which is the same as 8 bits in binary.\n\nWe can see that if we copy a variable to another one, the new one is basically a pointer to the old address. However, if a different variable is initialized with the same value as $x$, they will have a different address\n\n::: {#f8d1e115 .cell execution_count=2}\n``` {.python .cell-code}\ny = x\nprint(id(y)==id(x))\nprint(hex(id(y)) == hex(id(x)))\n\nz=4\nprint(hex(id(x)) == hex(id(z)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrue\nTrue\nTrue\n```\n:::\n:::\n\n\n### What about Arrays\n\nArrays are objects composed of elements of the same type and stored in continuous memory cells (homoguous memory). Computationally speaking they are more efficient than lists as we can access their elements just by using the location as a single pointer, since the first element will define the memory location of the others, while lists have a different pointer for each element.\n\n::: {#59f4b91b .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport sys\n\n# Create a Python list and a NumPy array with the same content\npy_list = [1, 2, 3, 4, 5]\nnp_array = np.array([1, 2, 3, 4, 5])\n\nprint(\"Python list memory addresses:\")\nfor i, item in enumerate(py_list):\n    print(f\"  Element {i}: value={item}, address={id(item)}\")\n\nprint(\"\\nNumPy array memory layout:\")\nprint(f\"  Base address of array: {np_array.__array_interface__['data'][0]}\")\nprint(\"  Item size in bytes:\", np_array.itemsize)\nfor i in range(len(np_array)):\n    address = np_array.__array_interface__['data'][0] + i * np_array.itemsize\n    print(f\"  Element {i}: value={np_array[i]}, address={address}\")\n\nprint(\"\\nTotal memory used by list elements:\", sum(sys.getsizeof(x) for x in py_list))\nprint(\"Total memory used by NumPy array:\", np_array.nbytes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPython list memory addresses:\n  Element 0: value=1, address=4379533928\n  Element 1: value=2, address=4379533960\n  Element 2: value=3, address=4379533992\n  Element 3: value=4, address=4379534024\n  Element 4: value=5, address=4379534056\n\nNumPy array memory layout:\n  Base address of array: 105553164846352\n  Item size in bytes: 8\n  Element 0: value=1, address=105553164846352\n  Element 1: value=2, address=105553164846360\n  Element 2: value=3, address=105553164846368\n  Element 3: value=4, address=105553164846376\n  Element 4: value=5, address=105553164846384\n\nTotal memory used by list elements: 140\nTotal memory used by NumPy array: 40\n```\n:::\n:::\n\n\nHere we have print the number of bytes needed to store the list or the array and we can see the difference.\n\n### An Important Property of Arrays: Strided View\n\nNow we have seen how arrays are stored and why it is more efficient than other data structure, it is important to discuss about a cool application of their continguous memory: the a strided representation.\n\nBasically, each array $n$ of shape $(I, J)$ will be stored in memory as a continuous sequence of number, its elements, and the shape will just clarify how to view the array.\n\nBasically we can define a view of our array as a final shape and an number of elements in the line before moving to the next row, column, called strides and expressed in bits (8 bits for example).\n\n::: {#93993ec1 .cell execution_count=4}\n``` {.python .cell-code}\nx = np.arange(6).reshape((2, 3))\nprint(x)\nprint(x.strides)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[0 1 2]\n [3 4 5]]\n(24, 8)\n```\n:::\n:::\n\n\nHere we have defined a two dimensional array that has to move 24 bits before changing row and 8 bits before changing column. With this information we can change the view just by changing the shape and the stride view:\n\n::: {#8faa67fa .cell execution_count=5}\n``` {.python .cell-code}\nnew_shape = (3, 2)\nnew_stride = (16, 8)\nnew_x = np.lib.stride_tricks.as_strided(x, new_shape, new_stride)\nprint(new_x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[0 1]\n [2 3]\n [4 5]]\n```\n:::\n:::\n\n\nWe are saying to move just two numbers before going to the next row and we have defined the final shape to have one more row and one less column.\n\nAnother important technique that arrays allow us to implement is to generate a bigger array just by tiling a smaller one with the *tile* operation.\n\n::: {#2a914de0 .cell execution_count=6}\n``` {.python .cell-code}\nx = np.eye(9)\nnum_rep = (3, 3)\nx_tiled = np.tile(x, num_rep)\nprint(x_tiled.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(27, 27)\n```\n:::\n:::\n\n\nHere we have repeated the identity matrix 3 times on the rows and 3 times in the columns.\n\nAll these operations are at the core of many machine learning architectures such as convolutional neural networks (CNNs), which we will implement in the last section of this post.\n\n## What about Tensors?\n\nTensors are the main datatype used by the most used ML library: PyTorch. Mathematically, tensors are any scalar, vector or matrices and we can say that they are the same as arrays (the computer science name for them). The main difference between NumPy's arrays and Torch's tensors is the implementation.\n\nSpecifically to ML, tensors are implemented in a way by which they support autograd and in this way backpropagation, the key concept for training a ML system, to read more about the implementation of backprop and autograd I suggest you the first lecture of the YouTube course from Karpathy here (add link).\n\nIf you don't have time to do the course, very briefly, every time you perform an operation between two tensors, torch will compute a computational graph that can be use to calculate the gradient relative to that tensor.\n\n::: {#99f8a183 .cell execution_count=7}\n``` {.python .cell-code}\nimport torch\n\na = torch.tensor(2.0, requires_grad=True)\ny = a**2\ny.backward()\n\nprint(a.grad.item())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n4.0\n```\n:::\n:::\n\n\nIn this way torch is able to compute the first derivative automatically if we specify that we want the gradient. Then the gradient is used to update the weights of the network depending on how the influence the loss.\n\nConcretely, this is achieved by defining a class method for each operation between tensors that allows you to save the numerical derivative for that tensor, and when you call the *backward()* method you simply compute the chain rule relative to that tensor.\n\n## Concrete Example: Building a Convolutional Kernel from Scratch\n\nWith these concepts in mind, we can finally see how they are used under the hood of the PyTorch library to train a convolutional neural network from scratch.\n\nImportant concepts: define the forward pass for a simple 2d conv layer, how to do it without for loops, use it in practice.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}