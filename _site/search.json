[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "ðŸ‘‹ Thomas Wyndham Bush\nStudent and Research Assistant in ML & Neuroscience\nIâ€™m a researcher exploring the intersection of machine learning, computational neuroscience, and language. I build tools for analyzing behavior, neural data, and AI explainability.\nThis site collects my articles, experiments, and thoughts as I learn and build in the open."
  },
  {
    "objectID": "posts/attention-einsum/index.html",
    "href": "posts/attention-einsum/index.html",
    "title": "Why Attention Is All You Need?",
    "section": "",
    "text": "The attention mechanism is arguably one of the most impactful innovations in the field of machine learning, if not the most. Its influence has extended beyond computer science, influencing many disciplines (AlphaFold, the field-changing model for protein structure prediction, is heavily dependent on the attention mechanism). As of this writing, the paper â€˜Attention Is All You Needâ€™, which introduced the transformer architecture based on attention mechanisms, has received over \\(185,000\\) citations.\nTo put that into perspective: the combined citation count of Darwinâ€™s â€˜On the Origin of Speciesâ€™, Hubel and Wieselâ€™s seminal work on the visual system, and Chomskyâ€™s â€˜Syntactic Structuresâ€™, a foundational text in linguistics, is still less than half of â€˜Attention Is All You Needâ€™.\nI still remember the first time that I encountered attention few years ago, it was during my first class of foundations of AI, and I found the idea of Query, Key and Values confusing and it took me a while to really grasp the core idea behind the attention mechanisms and why it works so well.\nWith this blog I aim to give you the insights that helped me understanding attention so that you can start build it yourself. Thus, I will first explain the core idea of attention, the difference between other representations and then I will implement it from scratch using a PyTorch and Einsum (a very cool library for linear algebra operation, to read it more see einsum is all you need"
  },
  {
    "objectID": "posts/attention-einsum/index.html#introduction",
    "href": "posts/attention-einsum/index.html#introduction",
    "title": "Why Attention Is All You Need?",
    "section": "",
    "text": "The attention mechanism is arguably one of the most impactful innovations in the field of machine learning, if not the most. Its influence has extended beyond computer science, influencing many disciplines (AlphaFold, the field-changing model for protein structure prediction, is heavily dependent on the attention mechanism). As of this writing, the paper â€˜Attention Is All You Needâ€™, which introduced the transformer architecture based on attention mechanisms, has received over \\(185,000\\) citations.\nTo put that into perspective: the combined citation count of Darwinâ€™s â€˜On the Origin of Speciesâ€™, Hubel and Wieselâ€™s seminal work on the visual system, and Chomskyâ€™s â€˜Syntactic Structuresâ€™, a foundational text in linguistics, is still less than half of â€˜Attention Is All You Needâ€™.\nI still remember the first time that I encountered attention few years ago, it was during my first class of foundations of AI, and I found the idea of Query, Key and Values confusing and it took me a while to really grasp the core idea behind the attention mechanisms and why it works so well.\nWith this blog I aim to give you the insights that helped me understanding attention so that you can start build it yourself. Thus, I will first explain the core idea of attention, the difference between other representations and then I will implement it from scratch using a PyTorch and Einsum (a very cool library for linear algebra operation, to read it more see einsum is all you need"
  },
  {
    "objectID": "posts/attention-einsum/index.html#the-core-idea-of-attention-and-why-it-works",
    "href": "posts/attention-einsum/index.html#the-core-idea-of-attention-and-why-it-works",
    "title": "Why Attention Is All You Need?",
    "section": "The Core Idea of Attention and Why It Works",
    "text": "The Core Idea of Attention and Why It Works\nAt its heart, the attention mechanism is conceptually simple. Suppose you have a sequence of elements N (e.g, words in a sentence), and you want to understand how each element relates to every other. The idea is to compare their similarities and then assign weights to each element reflecting its importance relative to the others. We could say:\n\nThe meaning of each element is defined not in isolation, but by how strongly it relates to all other elements in the sequence.\n\nTo implement this idea, the input sequence \\(S\\) is transformed into three different embeddings: Queries (Q), Keys (K) and Values (V). Then Queries and Keys are compared to measure similarity, and Values are weighted by these similarities to produce the final context-aware representations.\nBefore we dive into the actual implementation of attention, itâ€™s important to understand where it fits within the broader architecture of a Transformer. As shown in the original Attention Is All You Need paper, Transformers use a mechanism called multi-head attention, which means that the attention operation is applied multiple times in parallel.\n\n\n\nOverview Transformer Architecture\n\n\nEach head produces its own context-aware representation of the input, and these are then concatenated and passed through additional layers. For simplicity, the diagram below, and the rest of this post, will focus on how a single attention head works in isolation.\n\n\n\nSelf-Attention Diagram\n\n\nNow, this was actually the appropriate definition of â€˜Self-attentionâ€™ as we are comparing the sequence with itself, there are other kinds of attention, but I believe that itâ€™s helpful to start with self attention first as it is more intuitive and itâ€™s the most used mechanism nowadays, for an overview of the different kinds of attention you can visit A General Survey on Attention Mechanisms in Deep Learning, where they offer a nice taxonomy of the various attention mechanisms.\n\nWhat does really change between attention and MLP?\nThe main difference between attention and traditional architectures like MLPs is in how input information is processed. In an MLP, each input vector is transformed independently by the modelâ€™s weights. That is, the model learns a representation by projecting the input using a linear layer:\n\\[\\text{MLP}: h = W^TX\\]\nHere \\(X\\) is the input vector, and \\(W\\) is a learnable weight matrix. This transformation does not take into account the other elements of the input sequence. To model relationships between inputs, earlier architectures like Recurrent Neural Networks (RNNs) introduced recurrence, allowing the model to incorporate previous context step-by-step.\nIn contrast, attention directly models the relationships between inputs by comparing them to one another after the linear projections.\n\\[\\text{Attention}(Q, K, V): \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\\]\nAs we have seen, this operation allows each token to dynamically attend to the most relevant parts of the sequence, producing contextualized representations that reflect interactions between the inputs.\n\n\n\n\n\nAttention Output\n\n\n\n\n\n\nMLP Output\n\n\n\n\nThe difference is only in the operation after the linear transformations which allows to adjust the weights based on how to model input relations rather than input-weights relations."
  },
  {
    "objectID": "posts/attention-einsum/index.html#implementation",
    "href": "posts/attention-einsum/index.html#implementation",
    "title": "Why Attention Is All You Need?",
    "section": "Implementation",
    "text": "Implementation\nTo develop an attention module, we only need to import a couple of pythonâ€™s libraries, mostly to facilitate the backward pass during training, although if you are not familiar with backpropagation and how torch modules work under the hood, I strongly encourage you to try to implement some basics one yourself, see the Additional material for resources on this.\n\nimport torch\nfrom torch import affine_grid_generator, nn\nimport math\n\nAs we have seen, attention starts with the mapping of the input sequence (batch, n_tokens, dim_input) into three embeddings (Q, K, V) of shape: batch, n_tokens, dim_attention. This step is crucial as itâ€™s the only one where there are trainable parameters other than in the output layer.\n\n\n\nEmbedding Creation\n\n\nNext, we have to compute the affinities (similarity) between the query and key embeddings. We generate a matrix where each element of the sequence is compared to all the others and the values reflect the similarity between two elements as results of the dot product, which produces higher values for similar vectors.\nIn the code we can compute the dot product by using Einsum, basically given the query and key embedding \\(Q\\), \\(K\\) we compute: \\(Q \\cdot V = \\sum^c_{i=1} q_i * v_i\\). The output will be a matrix of shape: batch, dim_attention, dim_attention.\n\n\n\nCompute affinities\n\n\nThen, it is important to convert our affinities to weights that we can use to scale the value embedding. To do so, we use the softmax function to convert the values of the matrix into a probability distribution. The softmax formula is: \\(\\sigma(\\vec{z})_i = \\frac{\\exp(z_i)}{\\sum^K_{j=1} \\exp(z_j)}\\).\n\n\n\nGenerate attention weights\n\n\nFinally we compute the attention values by scaling the value vector by the attention weights and we return the output after passing it through a linear to map the attention dimension back to the same value of the input dimension.\n\n\n\nOutput Generation\n\n\nNow that we went through each step in detail we can put all of them together to built the initial diagram into a single python class called SelfAttentionModule. Below there is the full code to generate it.\n\n# we generate a random input\nx = torch.randn((12, 5, 10))\n#we then create a class:\nclass SelfAttentionModule(nn.Module):\n    def __init__(self, x_in, c):\n        super().__init__()\n        self.c = c\n        # we define the three linear layers\n        self.linear_q = nn.Linear(x_in, c)\n        self.linear_k = nn.Linear(x_in, c)\n        self.linear_v = nn.Linear(x_in, c)\n        self.linear_o = nn.Linear(c, x_in)\n\n    def forward(self, x:torch.Tensor)-&gt;torch.Tensor:\n        # we apply the linear transformations\n        Q = self.linear_q(x)\n        # scale Q\n        Q = Q / math.sqrt(self.c)\n        K = self.linear_k(x)\n        V = self.linear_v(x)\n        # calculate affinities:\n        affinities = torch.einsum('...kj, ...zi-&gt;...kz',Q, K)\n        # convert to weights\n        attn_weights = torch.softmax(affinities, dim=-1)\n        # get the attn_values:\n        attn_values = torch.einsum('...kz, ...zc-&gt;...kc', attn_weights, V)\n        attn_values = self.linear_o(attn_values)\n        return attn_values, attn_weights\n\n\nMasked Attention\nImportantly, since attention is often used in next token prediction tasks, if we were using the full attention weights across the entire sequence, weâ€™d allow each token to attend all future tokens, which would be like cheating. To prevent this, we apply a casual mask to the attention scores before computing the softmax. The mask will ensure that each token can only attend to the previous ones.\n\n\n\nAttention Mask\n\n\nThe mask is constructed by zerooing out the upper triangle of the attention matrix. These positions are then replaced with negatively infinity so that when passing through the softmax their contributions become zero. In PyTorch we can do it like this:\n\n#generate a random tensor\naffinities = torch.randn((4, 4))\n#create the mask\nmask = torch.tril(torch.ones_like(affinities)).bool()\n#apply the mask\naffinities = affinities.masked_fill(mask==0, float('-inf'))\n\n\n\nPositional Encoding in Attention\nAn interesting point that one could make at this stage is: how does attention keeps track of the order of the words? Well, it doesnâ€™t. Indeed, our implementation of attention would give use the same results regardless of the input sequence order, because it treats the input as a set not a sequence.\nTo solve we can tell the model where each token is in the sequence. That is the job of positional encoding.\nThe core idea behind positional encoding is to add to the token embedding a positional embedding, which encodes its position in the sequence. In this was we give the model a sense of location even when all the words in a sentence are the same but the order is different. For an overview of positional encoding I recommend you to visit Dive into DL chapter about that.\n\n\nComputational Efficiency on GPUs\nThe popularity of the attention mechanism is partly due to its extreme efficiency in processing input sequences. Specifically, attention allows us to compute a summary of the entire sequence for each element, in parallel. This makes it far more scalable than traditional sequence models like RNNs, which process inputs sequentially and are harder to parallelize.\nBefore diving into how attention is computed on a GPU, itâ€™s helpful to understand how GPUs work in general. When we define a vector, such as vect_1, a special function called kernel is launched on the GPU. The kernel takes each element of the vector and assigns it to a separate thread, which is run by a core in the GPU.\n\n\n\ngpu schematic\n\n\nNow, if we compute the dot product between two vectors, the GPU doesnâ€™t process it as one big operation. Instead, the kernel divides the computation into many smaller tasks, one for element, and assigns them to different cores. Each thread multiples one element from vect_1 with the corresponding element from the second vector. Finally, the results are combined using a reduction operation (sum), returning the final dot product.\nThis kind of parallelism is what gives GPUs their advantage over CPUs for task like attention, where operation across large sequences can be run simultaneously instead of one after the other.\nWhen applying this to attention, itâ€™s clear how much more efficient a forward pass can be compared to a recurrent neural network (RNN) processing the same input sequence. In attention, each element of the sequence can be handled in parallel because the computations for different positions are independent of one another. This allows the model to assign different parts of the attention mechanism to separate GPU cores, achieving high parallelism and speed. In contrast, an RNN must compute each step sequentially, since the output at each time step depends on the result from the previous one. This dependency makes it difficult to parallelize, significantly slowing down the forward pass.\n\n\n\nAttention vs RNNs"
  },
  {
    "objectID": "posts/attention-einsum/index.html#Additional-Material",
    "href": "posts/attention-einsum/index.html#Additional-Material",
    "title": "Why Attention Is All You Need?",
    "section": "Additional Material",
    "text": "Additional Material\n\nThe original paper: Attention is all you need\nTo review vector similarities and dot product visit: Vector Similarity Explained\nEinsum blog: Einsum is all you need\nCourse on Neural network basics from scratch: Neural Networks from Scratch in Pyton\nAnother blog about Transformer from Scratch"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "Dissection of a Tensor\n\n\n\n\n\n\n\n\nAug 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Attention Is All You Need?\n\n\n\n\n\n\n\n\nJul 17, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/dissection-array/index.html",
    "href": "posts/dissection-array/index.html",
    "title": "Dissection of a Tensor",
    "section": "",
    "text": "Before going through what is an array and why they are so much used in Computer Science or Machine Learning, we will have to do a quick excursus into how variables are stored in memory.\nWhen we declare a variable, that variable will be stored into the RAM (random access memory) of our computer. From this moment that variable, letâ€™s say \\(x\\) will have an address that will point to that memory location, a variable that stores the address of another one is called pointer.\n\nx = 4\nprint(hex(id(x)))\n\n0x102a23010\n\n\nHere we have assigned the value \\(4\\) to the variable x, what we will have is:\n\nId: it is the unique identity of that variable\nHexadecimal address: where that variable is located\n\nThe address is represented in hexadecimal format with 0-9 and A-F where each byte has an address. For example a two digit number as a limit of FF or 255 in decimal system which is the same as 8 bits in binary.\nWe can see that if we copy a variable to another one, the new one is basically a pointer to the old address. However, if a different variable is initialized with the same value as \\(x\\), they will have a different address\n\ny = x\nprint(id(y)==id(x))\nprint(hex(id(y)) == hex(id(x)))\n\nz=4\nprint(hex(id(x)) == hex(id(z)))\n\nTrue\nTrue\nTrue\n\n\n\n\nArrays are objects composed of elements of the same type and stored in continuous memory cells (homoguous memory). Computationally speaking they are more efficient than lists as we can access their elements just by using the location as a single pointer, since the first element will define the memory location of the others, while lists have a different pointer for each element.\n\nimport numpy as np\nimport sys\n\n# Create a Python list and a NumPy array with the same content\npy_list = [1, 2, 3, 4, 5]\nnp_array = np.array([1, 2, 3, 4, 5])\n\nprint(\"Python list memory addresses:\")\nfor i, item in enumerate(py_list):\n    print(f\"  Element {i}: value={item}, address={id(item)}\")\n\nprint(\"\\nNumPy array memory layout:\")\nprint(f\"  Base address of array: {np_array.__array_interface__['data'][0]}\")\nprint(\"  Item size in bytes:\", np_array.itemsize)\nfor i in range(len(np_array)):\n    address = np_array.__array_interface__['data'][0] + i * np_array.itemsize\n    print(f\"  Element {i}: value={np_array[i]}, address={address}\")\n\nprint(\"\\nTotal memory used by list elements:\", sum(sys.getsizeof(x) for x in py_list))\nprint(\"Total memory used by NumPy array:\", np_array.nbytes)\n\nPython list memory addresses:\n  Element 0: value=1, address=4339150768\n  Element 1: value=2, address=4339150800\n  Element 2: value=3, address=4339150832\n  Element 3: value=4, address=4339150864\n  Element 4: value=5, address=4339150896\n\nNumPy array memory layout:\n  Base address of array: 105553160355888\n  Item size in bytes: 8\n  Element 0: value=1, address=105553160355888\n  Element 1: value=2, address=105553160355896\n  Element 2: value=3, address=105553160355904\n  Element 3: value=4, address=105553160355912\n  Element 4: value=5, address=105553160355920\n\nTotal memory used by list elements: 140\nTotal memory used by NumPy array: 40\n\n\nHere we have print the number of bytes needed to store the list or the array and we can see the difference.\n\n\n\nNow we have seen how arrays are stored and why it is more efficient than other data structure, it is important to discuss about a cool application of their continguous memory: the a strided representation.\nBasically, each array \\(n\\) of shape \\((I, J)\\) will be stored in memory as a continuous sequence of number, its elements, and the shape will just clarify how to view the array.\nBasically we can define a view of our array as a final shape and an number of elements in the line before moving to the next row, column, called strides and expressed in bits (8 bits for example).\n\nx = np.arange(6).reshape((2, 3))\nprint(x)\nprint(x.strides)\n\n[[0 1 2]\n [3 4 5]]\n(24, 8)\n\n\nHere we have defined a two dimensional array that has to move 24 bits before changing row and 8 bits before changing column. With this information we can change the view just by changing the shape and the stride view:\n\nnew_shape = (3, 2)\nnew_stride = (16, 8)\nnew_x = np.lib.stride_tricks.as_strided(x, new_shape, new_stride)\nprint(new_x)\n\n[[0 1]\n [2 3]\n [4 5]]"
  },
  {
    "objectID": "posts/dissection-array/index.html#introduction-what-is-it-an-array",
    "href": "posts/dissection-array/index.html#introduction-what-is-it-an-array",
    "title": "Dissection of a Tensor",
    "section": "",
    "text": "Before going through what is an array and why they are so much used in Computer Science or Machine Learning, we will have to do a quick excursus into how variables are stored in memory.\nWhen we declare a variable, that variable will be stored into the RAM (random access memory) of our computer. From this moment that variable, letâ€™s say \\(x\\) will have an address that will point to that memory location, a variable that stores the address of another one is called pointer.\n\nx = 4\nprint(hex(id(x)))\n\n0x102a23010\n\n\nHere we have assigned the value \\(4\\) to the variable x, what we will have is:\n\nId: it is the unique identity of that variable\nHexadecimal address: where that variable is located\n\nThe address is represented in hexadecimal format with 0-9 and A-F where each byte has an address. For example a two digit number as a limit of FF or 255 in decimal system which is the same as 8 bits in binary.\nWe can see that if we copy a variable to another one, the new one is basically a pointer to the old address. However, if a different variable is initialized with the same value as \\(x\\), they will have a different address\n\ny = x\nprint(id(y)==id(x))\nprint(hex(id(y)) == hex(id(x)))\n\nz=4\nprint(hex(id(x)) == hex(id(z)))\n\nTrue\nTrue\nTrue\n\n\n\n\nArrays are objects composed of elements of the same type and stored in continuous memory cells (homoguous memory). Computationally speaking they are more efficient than lists as we can access their elements just by using the location as a single pointer, since the first element will define the memory location of the others, while lists have a different pointer for each element.\n\nimport numpy as np\nimport sys\n\n# Create a Python list and a NumPy array with the same content\npy_list = [1, 2, 3, 4, 5]\nnp_array = np.array([1, 2, 3, 4, 5])\n\nprint(\"Python list memory addresses:\")\nfor i, item in enumerate(py_list):\n    print(f\"  Element {i}: value={item}, address={id(item)}\")\n\nprint(\"\\nNumPy array memory layout:\")\nprint(f\"  Base address of array: {np_array.__array_interface__['data'][0]}\")\nprint(\"  Item size in bytes:\", np_array.itemsize)\nfor i in range(len(np_array)):\n    address = np_array.__array_interface__['data'][0] + i * np_array.itemsize\n    print(f\"  Element {i}: value={np_array[i]}, address={address}\")\n\nprint(\"\\nTotal memory used by list elements:\", sum(sys.getsizeof(x) for x in py_list))\nprint(\"Total memory used by NumPy array:\", np_array.nbytes)\n\nPython list memory addresses:\n  Element 0: value=1, address=4339150768\n  Element 1: value=2, address=4339150800\n  Element 2: value=3, address=4339150832\n  Element 3: value=4, address=4339150864\n  Element 4: value=5, address=4339150896\n\nNumPy array memory layout:\n  Base address of array: 105553160355888\n  Item size in bytes: 8\n  Element 0: value=1, address=105553160355888\n  Element 1: value=2, address=105553160355896\n  Element 2: value=3, address=105553160355904\n  Element 3: value=4, address=105553160355912\n  Element 4: value=5, address=105553160355920\n\nTotal memory used by list elements: 140\nTotal memory used by NumPy array: 40\n\n\nHere we have print the number of bytes needed to store the list or the array and we can see the difference.\n\n\n\nNow we have seen how arrays are stored and why it is more efficient than other data structure, it is important to discuss about a cool application of their continguous memory: the a strided representation.\nBasically, each array \\(n\\) of shape \\((I, J)\\) will be stored in memory as a continuous sequence of number, its elements, and the shape will just clarify how to view the array.\nBasically we can define a view of our array as a final shape and an number of elements in the line before moving to the next row, column, called strides and expressed in bits (8 bits for example).\n\nx = np.arange(6).reshape((2, 3))\nprint(x)\nprint(x.strides)\n\n[[0 1 2]\n [3 4 5]]\n(24, 8)\n\n\nHere we have defined a two dimensional array that has to move 24 bits before changing row and 8 bits before changing column. With this information we can change the view just by changing the shape and the stride view:\n\nnew_shape = (3, 2)\nnew_stride = (16, 8)\nnew_x = np.lib.stride_tricks.as_strided(x, new_shape, new_stride)\nprint(new_x)\n\n[[0 1]\n [2 3]\n [4 5]]"
  },
  {
    "objectID": "posts/dissection-array/index.html#what-about-tensors",
    "href": "posts/dissection-array/index.html#what-about-tensors",
    "title": "Dissection of a Tensor",
    "section": "What about Tensors?",
    "text": "What about Tensors?"
  },
  {
    "objectID": "posts/dissection-array/index.html#concrete-example-building-a-convolutional-kernel-from-scratch",
    "href": "posts/dissection-array/index.html#concrete-example-building-a-convolutional-kernel-from-scratch",
    "title": "Dissection of a Tensor",
    "section": "Concrete Example: Building a Convolutional Kernel from Scratch",
    "text": "Concrete Example: Building a Convolutional Kernel from Scratch"
  }
]