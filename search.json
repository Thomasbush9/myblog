[
  {
    "objectID": "posts/attention-einsum/index.html",
    "href": "posts/attention-einsum/index.html",
    "title": "Understanding Attention with Einsum",
    "section": "",
    "text": "The attention mechanism is arguably one of the most impactful innovations in the field of machine learning, if no the most. Its influence has extended beyond computer science, influencing many disciplines (AlphaFold, the field-changing model for protein structure prediction, is heavily dependent on the attention mechanism). As of this writing, the paper â€˜Attention Is All You Needâ€™, which introduced the transformer architecture based solely on attention mechanisms, has received over \\(185,000\\) citations.\nTo put that into perspective: the combined citation count of Darwinâ€™s â€˜On the Origin of Speciesâ€™, Hubel and Wieselâ€™s seminal work on the visual system, and Chomskyâ€™s â€˜Syntactic Structuresâ€™, a foundational text in linguistics, is still less than half of â€˜Attention Is All You Needâ€™."
  },
  {
    "objectID": "posts/attention-einsum/index.html#introduction",
    "href": "posts/attention-einsum/index.html#introduction",
    "title": "Understanding Attention with Einsum",
    "section": "",
    "text": "The attention mechanism is arguably one of the most impactful innovations in the field of machine learning, if no the most. Its influence has extended beyond computer science, influencing many disciplines (AlphaFold, the field-changing model for protein structure prediction, is heavily dependent on the attention mechanism). As of this writing, the paper â€˜Attention Is All You Needâ€™, which introduced the transformer architecture based solely on attention mechanisms, has received over \\(185,000\\) citations.\nTo put that into perspective: the combined citation count of Darwinâ€™s â€˜On the Origin of Speciesâ€™, Hubel and Wieselâ€™s seminal work on the visual system, and Chomskyâ€™s â€˜Syntactic Structuresâ€™, a foundational text in linguistics, is still less than half of â€˜Attention Is All You Needâ€™."
  },
  {
    "objectID": "posts/attention-einsum/index.html#the-core-idea-of-attention",
    "href": "posts/attention-einsum/index.html#the-core-idea-of-attention",
    "title": "Understanding Attention with Einsum",
    "section": "The Core Idea of Attention",
    "text": "The Core Idea of Attention"
  },
  {
    "objectID": "posts/attention-einsum/index.html#implementation",
    "href": "posts/attention-einsum/index.html#implementation",
    "title": "Understanding Attention with Einsum",
    "section": "Implementation",
    "text": "Implementation"
  },
  {
    "objectID": "posts/attention-einsum/index.html#visualization",
    "href": "posts/attention-einsum/index.html#visualization",
    "title": "Understanding Attention with Einsum",
    "section": "Visualization",
    "text": "Visualization"
  },
  {
    "objectID": "posts/attention-einsum/index.html#conclusion-and-suggested-material",
    "href": "posts/attention-einsum/index.html#conclusion-and-suggested-material",
    "title": "Understanding Attention with Einsum",
    "section": "Conclusion and Suggested Material",
    "text": "Conclusion and Suggested Material\n\nimport torch\nimport math\n\n\nx = torch.randn((12, 10, 5))"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "ðŸ‘‹ Thomas Wyndham Bush\nStudent and Research Assistant in ML & Neuroscience\nIâ€™m a researcher exploring the intersection of machine learning, computational neuroscience, and language. I build tools for analyzing behavior, neural data, and AI explainability.\nThis site collects my articles, experiments, and thoughts as I learn and build in the open."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "Understanding Attention with Einsum\n\n\n\n\n\n\n\n\nJul 17, 2025\n\n\n\n\n\nNo matching items"
  }
]